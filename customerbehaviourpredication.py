# -*- coding: utf-8 -*-
"""CustomerBehaviourPredication.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1e9rnXD_b0Yp98MJU0qg9cZYpPMI5TKJY
"""

import pandas as pd
import numpy as np
pd.set_option('display.max_columns', None)
import matplotlib.pyplot as plt
import seaborn as sn
from datetime import date, datetime
from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler # For handling categorical column and scaling numeric columns

# Libraries for clustering and evaluation
from sklearn.cluster import KMeans
from yellowbrick.cluster import KElbowVisualizer
from sklearn.metrics import silhouette_score

import warnings
warnings.filterwarnings("ignore")

df=pd.read_excel('/content/marketing_campaign.xlsx')

df.head(10)
df.shape
df.info()
df.isnull().sum()
df.describe()
df.duplicated().sum()

df.rename(columns={'MntGoldProds':'MntGoldProducts'}, inplace=True)
df['Year_Birth']=pd.to_datetime(df['Year_Birth'], format='%Y')
df['Dt_Customer']=pd.to_datetime(df['Dt_Customer'])
df['Income'].skew()

df['Income'].fillna(df['Income'].median(),inplace=True)
df.isnull().sum()
df['Education'].value_counts()
df['Marital_Status'].value_counts()
df['Dt_Customer'].dt.year.min(), df['Dt_Customer'].dt.year.max()

df['Age']=(df['Dt_Customer'].dt.year.max())-(df['Year_Birth'].dt.year)
df['Year_Customer']=(df['Dt_Customer'].dt.year.max())-(df['Dt_Customer'].dt.year)
df['Days_Customer']=(df["Dt_Customer"].max()) - (df['Dt_Customer'])
df['TotalMntSpent'] = df['MntWines'] + df['MntFruits'] + df['MntMeatProducts'] + df['MntFishProducts'] + df['MntSweetProducts'] + df['MntGoldProducts']
df['TotalNumPurchases'] = df['NumWebPurchases'] + df['NumCatalogPurchases'] + df['NumStorePurchases'] + df['NumDealsPurchases']
df['Total_Acc_Cmp'] = df['AcceptedCmp1'] + df['AcceptedCmp2'] + df['AcceptedCmp3'] + df['AcceptedCmp4'] + df['AcceptedCmp5'] + df['Response']
df['Year_Joined'] = df['Dt_Customer'].dt.year
df['Month_Joined'] = df['Dt_Customer'].dt.strftime("%B")
df['Day_Joined'] = df['Dt_Customer'].dt.day_name()
df['Age_Group'] = pd.cut(x = df['Age'], bins = [17, 24, 44, 64, 150],
                         labels = ['Young adult','Adult','Middel Aged','Senior Citizen'])
# Total children living in the household
df["Children"] = df["Kidhome"] +  df["Teenhome"]
df["Partner"]=df["Marital_Status"].replace({"Married":"Yes", "Together":"Yes", "Absurd":"No", "Widow":"No", "YOLO":"No", "Divorced":"No", "Single":"No","Alone":"No"})

#Segmenting education levels in three groups
df["Education_Level"]=df["Education"].replace({"Basic":"Undergraduate","2n Cycle":"Undergraduate", "Graduation":"Graduate", "Master":"Postgraduate", "PhD":"Postgraduate"})
df.drop(['ID','Z_CostContact','Z_Revenue','Year_Birth','Dt_Customer'], axis=1, inplace=True)
df['Days_Customer'] = df['Days_Customer'].dt.days.astype('int16')

df1=df.copy()
df1.drop(['Education','Marital_Status','Year_Customer','Year_Joined','Month_Joined','Day_Joined'], axis=1, inplace=True)

num_col = df1.select_dtypes(include = np.number).columns
for col in num_col:
    q1 = df1[col].quantile(0.25)
    q3 = df1[col].quantile(0.75)
    iqr = q3-q1
    ll = q1-(1.5*iqr)
    ul = q3+(1.5*iqr)
    for ind in df1[col].index:
        if df1.loc[ind,col]>ul:
            df1.loc[ind,col]=ul
        elif df1.loc[ind,col]<ll:
            df1.loc[ind,col]=ll
        else:
            pass
print("Outliers have been taken care of")

subset = df1[['Income','Kidhome','Teenhome','Age','Partner','Education_Level']]
print('This is the data we will use for clustering:')
subset.head()

from sklearn.pipeline import make_pipeline
from sklearn.compose import ColumnTransformer

num_cols = ['Income','Age']
numeric_pipeline = make_pipeline(StandardScaler())

ord_cols = ['Education_Level']
ordinal_pipeline = make_pipeline(OrdinalEncoder(categories=[['Undergraduate','Graduate','Postgraduate']]))

nom_cols = ['Partner']
nominal_pipeline = make_pipeline(OneHotEncoder())

# stack your pipelines in column transformer
transformers=ColumnTransformer(transformers=[('num',numeric_pipeline,num_cols),
                                             ('ordinal',ordinal_pipeline,ord_cols),
                                             ('nominal',nominal_pipeline,nom_cols)])

transformers

transformed = transformers.fit_transform(subset)
print('Data has been Transformed')

plt.figure(figsize=(12, 8))
elbow_graph = KElbowVisualizer(KMeans(random_state=43), k=10)
elbow_graph.fit(transformed)
elbow_graph.show()

kmeans = KMeans(n_clusters=5, random_state=42)
subset['Clusters'] = kmeans.fit_predict(transformed) #fit the data and adding back clusters to the data in clusters column

subset.head()

plt.figure(figsize=(12, 8))
sn.countplot(x='Clusters', data=subset)
plt.title('Customer Distribution Within Clusters')
plt.show()

# create list of categories
count_cols= ['Kidhome','Teenhome','Partner','Education_Level']

_, ax1 = plt.subplots(2,2, figsize=(25,22))

for i, col in enumerate(count_cols):
    sn.countplot(x='Clusters', data=subset, ax=ax1[i//2, i%2],hue=col,)

plt.show()

# Lets findout income of customers with in clusters
plt.figure(figsize=(12, 8))
sn.barplot(x=subset["Clusters"], y=subset["Income"])
plt.title("Income vs Clusters", size=15)
plt.show()

# import required libraries
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.ensemble import GradientBoostingClassifier

# separate features and target column
x = subset.drop('Clusters', axis=1)
y = subset['Clusters']

# create train and test data
x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.3, random_state=42)

# adding GradientBoostingClassifier to transformer pipeline
final_pipeline = make_pipeline(transformers, GradientBoostingClassifier())

# fit the data to new pipeline & model
final_pipeline.fit(x_train, y_train)

# check the accuracy of our model
final_pipeline.score(x_test, y_test)